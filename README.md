# ğŸ§  Deep Q-Learning Agent on Custom FrozenLake (3x3 Grid)

This is a minimalist implementation of a Deep Q-Network (DQN) agent that learns to navigate a custom-made 3x3 FrozenLake-style environment. The environment has a start state, goal state, and a hole (failure state). The agent learns to reach the goal while avoiding the hole using reinforcement learning.

---

## ğŸ“Œ Features

- ğŸ” **Fully Custom GridWorld**: 3x3 deterministic environment built from scratch.
- ğŸ§  **Deep Q-Learning**: Uses PyTorch to implement a neural network for Q-value approximation.
- ğŸ§ª **Training Loop**: Trains the agent over multiple episodes with epsilon-greedy exploration.
- ğŸ“ˆ **Logging**: Displays training progress and final policy values.
- âš¡ **Lightweight**: No external environment dependencies (like OpenAI Gym).

---

## ğŸ—ºï¸ Environment Design

```
S - Start  
G - Goal (reward = +1)  
H - Hole (reward = -1)  
. - Safe cell (reward = 0)

Grid layout:  
[ S, ., . ]  
[ ., H, . ]  
[ ., ., G ]
```

---

## ğŸ“¦ Dependencies

- Python 3.8+
- PyTorch

Install dependencies using:

```bash
pip install torch
```

---

## ğŸš€ How to Run

```bash
python DQN.py
```


---

## ğŸ§  Neural Network Architecture

- **Input size**: 9 (flattened one-hot vector of grid position)
- **Hidden layers**: 64 â†’ 32 neurons with ReLU
- **Output**: 4 Q-values (one per action: up, down, left, right)

---
 
---

## ğŸ“Œ Policy Interpretation

The policy is extracted by taking the `argmax` over the Q-values for each state. This shows what action the agent would take in each state after learning.

---

## ğŸ“š Learning Details

| Parameter      | Value      |
|----------------|------------|
| Episodes       | 30         |
| Gamma (Î³)      | 0.9        |
| Epsilon (Îµ)    | 1          |
| LR             | 0.01       |
| Optimizer      | Adam       |

---

## ğŸ§ª Future Improvements

- Add generalization testing with randomized goal/hole.
- Visualize learning with matplotlib or a UI.
- Extend to larger grids (e.g., 4x4 or 5x5).
- Test performance under stochastic transitions.

---

## âœ¨ Output Sample (After Training)
```
Optimal Action Grid:
 â†’  â†’  â†“
 â†“  H  â†“
 â†’  â†’  G
```
---

## ğŸ¤ Contributing

Feel free to fork and play with the environment setup or agent architecture. Open to suggestions and pull requests.

---

## ğŸ“œ License

This project is open-source under the MIT License.
